## GFS简介

GFS全称是 Google File system，是一个大规模分布式数据密集型应用的，可扩展的分布式文件系统，它虽然运行在廉价普通的机器上，但是提供了容错和向海量用户交付高集成性能。

GFS有和早起分布式文件系统一样的设计目标：

+ performance：性能
+ scalability：可扩展性
+ reliability：可靠性
+ availability：有效性

但是，它的设计驱动对于应用工作负载和技术环境关键观察，无论是当前还是未来，都反映了和早起文件系统显著的不同假设。

1. 组件故障是常态。
2. 传统标准下的文件都很大，GB级别的文件很常见。
3. 大部分文件因为追加新的数据从而引起突变，而不是覆盖现有数据。
4. 通过协同设计(co-designing)应用程序和文件系统API，增加灵活性让整个系统受益。

多个GFS集群为不同的目标被部署，最大的有1000个存储节点，接近300TB的磁盘存储，并且被不同机器上的数百个客户端大量持续访问。

## 设计概要

根据上面的4点假设，可以了解到更多的细节：

+ 首先，系统建立在许多廉价的硬盘上面，所以会经常出现错误的部件。
+ 系统存储了适当的大文件。
+ 工作负载(workloads)主要是两种读类型组成：
  + 大量流式读
  + 少量随机读
+ 工作负载还有大量顺序写，添加到到文件中。
+ 系统必须高效的实现定义清楚的语义，来支持多个客户端并发追加数据到同一个文件。
+ 高可持续带宽比低延迟更加重要。

## 设计总结

总体上来看，系统组件容易出现故障，是因为使用了上百上千台廉价普通的硬件组成起来的存储机器，并且被大量的客户机访问，在使用过程中，组件的数量和质量几乎确定了在任何时间都有一些组件失效，甚至有些无法从当前错误中恢复回来，有应用bugs，操作系统bugs和认为错误，还有硬盘错误，内存，连接器，网络和电源供应引起的问题。

为了解决这些问题，就需要把：

+ 持续监控
+ 错误探测
+ 容错
+ 自动恢复

上面这些功能集成到系统中，它必须在日常生活持续自我监控和探测，容错，还有及时的从故障中恢复。

传统标准下的文件很大，GB级别的都很正常，通常这些文件都包含了许多应用程序，比如Web文档，当需要处理快速增长数据集组成的数十亿对象的时候，在使用数十亿约KB大小的文件去管理就不适合了，所以设计假设和参数，比如IO操作，块大小这些就必须被重新设计，小文件也必须要支持，但是不用专门优化。

## 接口

GFS并没有实现标准的API，比如POSIX，但是提供了类似的文件系统接口，文件在目录中按照层次结构组织，路径名字表示。

支持常规操作：

+ 创建
+ 删除
+ 打开
+ 关闭
+ 读取文件
+ 写文件

除此之外，GFS还支持了快照和记录追加操作，快照使用低代价指创建一份文件副本或目录树，记录追加操作允许大量客户端并发追加数据到相同文件，并且保证每个客户端的追加原子性。

实现结果的是多路归并和多个客户端同时追加的生产者消费者队列，不需要额外锁非常有用，这些文件对构造分布式应用非常有价值。

## GFS 架构

+ chunk：块
+ chunk server：块服务器
+ chunk handle 块句柄

![GFS Architecture](/home/cola/CScourse/MIT6.824lab2021/paperNotes/GFS Architecture.png)

一个GFS集群由一个master和多个块服务器组成，并且可以被多个客户端访问，如上图所示，每个客户端都是运行用户级进程的普通Linux机器。

只要机器资源允许，就可以在同一个机器上运行块服务器和客户端，并且，由于运行的可能是片状应用(flaky application)代码，从而导致可靠性降低，这是可以接受的。

文件被分为固定的大小块，每一块都是通过一个不变的，全局唯一的64位块句柄来识别，创建块的时候，通过master来分配句柄。

块服务器在本地磁盘上存储块为Linux文件，通过块句柄来和字节区间读写块数据，为了可靠性，每个块都在多个快服务器上存有副本，默认情况下，一般存储三份，但是用户可以为文件命名空间不同区域指定副本级别。

master维护所有的文件系统元数据，它包括如下内容：

+ namespace：命名空间
+ access control information：访问控制信息
+ The map-ping from files to chunks and the current locations of chunks：文件到块的映射和当前块的位置。

比如块租约管理(chunk lease management)，孤儿块(orphaned chunks)，垃圾回收(garbage collection)和块在块服务器空间的迁移，master周期性的发送心跳信息(HeartBeat)来传递指令和收集状态，GFS客户端代码链接到每个分布式应用，他实现了文件系统API，并且根据应用程序行为来和master和块服务器通信来读写数据。

客户端和master通过元数据进行交互，但是所有的数据承载传输都是通过块服务器，因为没有实现POSIX API，所以就不要深入理解linux vnode层了。

客户端和块服务器都不缓存文件数据，因为大部分流式应用都是读超大文件或者太巨大而无法缓存的工作数据集，所以客户端缓存是没有任何好处的，没有了缓存也就通过了消除缓存一致性问题。

简化了客户端和系统整体，不过客户端会缓存元数据，块服务器不需要缓存文件数据，因为块被存储为本地文件，所以linux的缓冲区缓存已经在内存中存储了经常被访问的数据。

## Single Master 

只有一个master最大程度上简化了设计的复杂度，使得master能够使用全局信息进行复杂的块配置和副本决策，然而，我们必须最小化master读写的参与，使其不成为瓶颈。

客户端不会通过master来进行读写操作，客户端会询问master应该和那个块服务器进行联系，它在有限的时间内缓存这个信息并且后面许多顺序操作都直接和块服务器进行交互。

这里举一个简单的读操作写来解释交互过程，首先，使用固定大小的块，客户端讲应用指定文件名和字节偏移量转换为文件中的块index。

然后，它向master发送一个包含文件名和块index的请求，master回应相对应的块handle和副本的位置，客户端使用文件名和块index作为键值缓存这个信息。

客户端最后向其中一个副本，可能是最近的一个，发送请求，这个请求包含了块handle和块中的字节区间。

后续读取相同块的时候，就不再需要客户端-master交互了，直到缓存信息过去或者文件重新被打开。

实际上，客户端一般请求一次会请求多个块，master也可以在回复中包含紧跟着那些被请求的块信息。

## 块大小

块的尺寸是一个关键的设计信息之一，google选了64MB作为块的大小，它比常规的文件系统block size大很多，每个块副本作为一个普通的linux文件存储在一台块服务器上，仅在需要的时候扩展。

惰性空间分配避免了内存碎片造成的空间浪费，这可能是反对大块64MB的原因。

大块size有一些重要的优势，首先，它减少了客户端和master交互的需求，因为在同样的块上进行读写，只需要向master请求一次获取块的信息，之后就不需要了。

因为应用基本上顺序读写大文件，甚至对于小型文件随机读，客户端可以方便的缓存一个多TB工作集的所有块位置信息，因为对于一个大块客户端很可能对一个给定的块持续执行多次操作，它可以通过对一个块服务器在一定时间内保持一个持久的TCP连接来减少网络开销（network overhead）。最后，它减少了存储在master上的元数据大小。这使得我们可以把元数据存储在内存。

但是大块size还有一些缺点，一个小文件由少量块组成，可能是一个块组成，如果许多客户端访问相同的文件，存储这些块的块服务器可能就会变成热点。

## 元数据

## 一致性模型

## 系统交互

## master操作

## 容错和诊断

## 测量 MEASUREMENTS

## 经验 EXPERIENCES

## 相关工作

## 结论

## 一些比较重要的信息

+ lazy space allocation 惰性空间
+ file system 文件系统